{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standard library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "#Image processing packages\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "## Keras + Tensorflow\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(r'./ships-in-satellite-imagery/shipsnet_v2.json')\n",
    "dataset = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detailed description of the final test code. Slight modification for use in scan image.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.array(dataset['data']).astype('uint8')\n",
    "labels = np.array(dataset['labels']).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data / 255.\n",
    "\n",
    "# transpose the position according to the axis order specify in the list\\\n",
    "# [0,2,3,1] -- shape[0], shape[2], shape [3], shape[1]\n",
    "# reshape --> -1 means keeping the dimension value at axis[0]\n",
    "\n",
    "x = x.reshape([-1, 3, 80, 80]).transpose([0,2,3,1])\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(labels, num_classes=2)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN model. Starting point from keras tutorial: VGG-like convnet\n",
    "# https://keras.io/getting-started/sequential-model-guide/#training\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=(80, 80, 3), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "## batch normalization -- code\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Potential Third Layer\n",
    "# model.add(Conv2D(128, (3, 3), padding=\"same\", activation='relu'))\n",
    "# model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "## FC layer 1\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "## FC layer 2 with softmax classifer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "## compile a model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001, momentum=0.9, nesterov=True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "\n",
    "history = model.fit(x, y, batch_size=64, epochs=150, validation_split=0.2)\n",
    "\n",
    "train_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_time = train_end - train_start\n",
    "print ('CNN model train time = {}'.format(train_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save train history dict and load train history dict to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./cnn_model/classiferCNN/model2_hist.pkl', 'wb') as file_name:\n",
    "    pickle.dump(history.history, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./cnn_model/classiferCNN/model2_hist.pkl','rb') as file_name:\n",
    "    modelnew_hist = pickle.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save model and load model to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('./cnn_model/classiferCNN/model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelnew = load_model('./cnn_model/classiferCNN/model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## plotting the history of the CNN training\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
    "\n",
    "ax[0].plot(modelnew_hist['acc'])\n",
    "ax[0].plot(modelnew_hist['val_acc'])\n",
    "ax[0].set_title('model accuracy')\n",
    "ax[0].set_ylabel('accuracy')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "ax[1].plot(modelnew_hist['loss'])\n",
    "ax[1].plot(modelnew_hist['val_loss'])\n",
    "ax[1].set_title('model loss')\n",
    "ax[1].set_ylabel('loss')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### scanning of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_window(image, stepsize , winsize):\n",
    "    # image is always specify by height first then width\n",
    "    \n",
    "    # slide a window across the satellite image\n",
    "    for y in range(0, image.shape[0], stepsize[0]):\n",
    "        for x in range(0, image.shape[1], stepsize[1]):\n",
    "            # yield to generate the coordinates and window image for use in the classifier\n",
    "            # as we scan through the image\n",
    "            yield (x, y, x + winsize[1], y + winsize[0], image[y:y + winsize[0], x:x + winsize[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_max_suppression(bound_box, area_overlap_threshold):\n",
    "    \n",
    "    # bound_box => np.array of bounding box coordinate[(x1,y1,x2,y2),(),()] for numpy slicing\n",
    "    boxes = np.array(bound_box)\n",
    "    \n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    " \n",
    "    # initialize a list for pick boxes coordinates\n",
    "    pick = []\n",
    " \n",
    "    # x1, y1 top left point of each bounding box\n",
    "    # x2, y2 bottem right point of each bounding box\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    " \n",
    "   \n",
    "    # Default bounding box area is the same as the search window --> 80*80 == 6400\n",
    "    area = 6400.0\n",
    "    \n",
    "    # Sort the index of the bounding boxes by y2 (bottem right)\n",
    "    ind = np.argsort(y2)\n",
    " \n",
    "    while len(ind) > 0:\n",
    "        end = len(ind) - 1\n",
    "        i = ind[end] # get the last index of the ind list for coordinates (x1,y1,x2,y2)\n",
    "        pick.append(i) # pick the last bounding box in the list (bottom right most bounding box in the full sat image)\n",
    "        suppress = [end] # place the index of last bounding box in suppress\n",
    "        \n",
    "        # At the end of each for loop, the index of last bounding box will be deleted from ind list \n",
    "        # as it is already been place in the pick list\n",
    "        \n",
    "        # Each loop will try to find neighbouring boxes with high overlapping area with the last bounding box in the list\n",
    "        # Once found, suppress these neighbouring boxes and only keep the bottom right most\n",
    "        \n",
    "        for coord in range(0, end):\n",
    "            j = ind[coord]\n",
    "            # Loop through all the remaining bounding boxes in ind list\n",
    "            \n",
    "            # find the largest (x, y) coordinates for the start of the bounding box\n",
    "            # find smallest (x, y) coordinates for the end of the bounding box\n",
    "            \n",
    "            # This is used to calculate the overlapping area\n",
    "            xx1 = max(x1[i], x1[j])\n",
    "            yy1 = max(y1[i], y1[j])\n",
    "            xx2 = min(x2[i], x2[j])\n",
    "            yy2 = min(y2[i], y2[j])\n",
    " \n",
    "            # compute the width and height of the bounding box\n",
    "            \n",
    "            # Note that the min, max selection of coordinates is with reference to the last bounding box in the image\n",
    "            # x[i] is the last bounding box with the highest x1,y1,x2,y2 value when compared to non-overlapping boxes.\n",
    "            # Therefore:\n",
    "            \n",
    "            # xx2 - xx1 will be negative and w = 0\n",
    "            # yy2 - yy1 will be positive and h = 0\n",
    "            \n",
    "            # if there is no overlapping of the boxes i, j.\n",
    "        \n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    " \n",
    "            # compute the ratio of overlap\n",
    "            # Bounding box area is only 1 size of area 6400\n",
    "            overlap = (w * h) / area\n",
    "        \n",
    "            # check if the overlap ratio exceed a certain value. \n",
    "            # if so, put it into the suppress list\n",
    "            if overlap > area_overlap_threshold:\n",
    "                suppress.append(coord)\n",
    " \n",
    "        # delete all index in suppress list from ind list.\n",
    "        ind = np.delete(ind, suppress)\n",
    " \n",
    "    return boxes[pick]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "satimage_scan = cv2.imread('./ships-in-satellite-imagery/test_v2/sfbay_1.png')\n",
    "satimage_result = cv2.imread('./ships-in-satellite-imagery/test_v2/sfbay_1.png')\n",
    "satimage_nms = cv2.imread('./ships-in-satellite-imagery/test_v2/sfbay_1.png')\n",
    "print (satimage_scan.shape[0])\n",
    "print (satimage_scan.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(winH, winW) = (80, 80)\n",
    "(stepY,stepX) = (10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop sliding window over the enitre satellite image\n",
    "\n",
    "pred_class = []\n",
    "pred_prob = []\n",
    "win_img = []\n",
    "win_coordinate = []\n",
    "\n",
    "ship_pred_class = []\n",
    "ship_pred_prob = []\n",
    "ship_img = []\n",
    "ship_cooridinate = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "## time for the sliding window to scan the whole satellite image\n",
    "scan_start = time.time()\n",
    "\n",
    "for (x1, y1, x2, y2, window) in sliding_window(satimage_scan, stepsize=(stepY,stepX), winsize=(winH, winW)):\n",
    "    \n",
    "    # If the window does not meet our desired window size, continue \n",
    "    # For windows reaching the edge, ignoring those windows not the same size as the specified winsize)\n",
    "    \n",
    "    if window.shape[0] != winH or window.shape[1] != winW:\n",
    "        continue\n",
    "\n",
    "    # Classifier Code\n",
    "    # Classify content within the sliding window\n",
    "    # input a new dims on the window so that it can be input into the CNN classifier (take in only a tensor)\n",
    "    target_region = np.expand_dims(window, axis=0)\n",
    "    classes = modelnew.predict_classes(target_region, train=False)\n",
    "    x_proba = modelnew.predict(target_region)\n",
    "   \n",
    "    pred_class.append(classes[0])\n",
    "    pred_prob.append(x_proba)\n",
    "    win_img.append(window)\n",
    "    win_coordinate.append((x1,y1,x2,y2))\n",
    "    \n",
    "    # classes [0] = 1 is ship\n",
    "    # classes [0] = 0 is not ship\n",
    "    \n",
    "    if classes[0] == 1:\n",
    "        \n",
    "        cv2.imwrite(\"./results/classiferCNN/winimg2/rimg_{}.png\".format(i), window)\n",
    "        \n",
    "        ship_pred_class.append(classes[0])\n",
    "        ship_pred_prob.append(x_proba)\n",
    "        ship_img.append(window)\n",
    "        ship_cooridinate.append((x1,y1,x2,y2))\n",
    "        \n",
    "        # cv2.namedWindow(\"Window\", cv2.WINDOW_NORMAL) \n",
    "        # draw a rectange green box at the pixel coordinate (x, y) and (x + winW, y + winH) with box line thickness of 2 pixel\n",
    "        cv2.rectangle(satimage_result, (x1, y1), (x1 + winW, y1 + winH), (0, 255, 0), 2) \n",
    "        #cv2.imshow(\"Windows\",satimage)\n",
    "        \n",
    "        # Save the big satellite image for gif generation\n",
    "        cv2.imwrite(\"./results/classiferCNN/gif2/img_{}.png\".format(i), satimage_result)\n",
    "        \n",
    "        # Save the window image that has a bounding box\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        #cv2.waitKey(1)\n",
    "        #time.sleep(0.025)\n",
    "        \n",
    "        ### save png files\n",
    "    #else:\n",
    "        #cv2.waitKey(1)\n",
    "        #time.sleep(0.025)\n",
    "\n",
    "\n",
    "# Final result of satimage_result\n",
    "# Everytime satimage_result will update with more green boxes\n",
    "cv2.imwrite(\"./results/classiferCNN/final_result/detected2.png\", satimage_result)\n",
    "cv2.destroyAllWindows()\n",
    "                          \n",
    "scan_end = time.time()\n",
    "scan_time = scan_end - scan_start\n",
    "\n",
    "print(scan_time)\n",
    "\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result_data = {'pred_class': pred_class, 'pred_prob': pred_prob, 'win_img': win_img, 'win_coordinate': win_coordinate}\n",
    "predict_ship = {'ship_pred_class': ship_pred_class, 'ship_pred_prob': ship_pred_prob, 'ship_img': ship_img, 'ship_cooridinate': ship_cooridinate}\n",
    "\n",
    "with open('./results/classiferCNN/result_data2.pkl', 'wb') as file_name:\n",
    "    pickle.dump(result_data, file_name)\n",
    "    \n",
    "with open('./results/classiferCNN/predict_ship2.pkl', 'wb') as file_name:\n",
    "    pickle.dump(predict_ship, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## NMS to suppress overlapping bounding boxes\n",
    "# area overlap ratio = 0.5, suppress boxes that overlap more then half the area\n",
    "\n",
    "print('NMS')\n",
    "satimage_nms = cv2.imread('./ships-in-satellite-imagery/test_v2/sfbay_1.png')\n",
    "picked_box = non_max_suppression(ship_cooridinate, area_overlap_threshold = 0.1)\n",
    "\n",
    "for (bbx1, bby1, bbx2, bby2) in picked_box:\n",
    "    cv2.rectangle(satimage_nms, (bbx1, bby1), (bbx2,bby2), (0, 0, 255), 2) \n",
    "\n",
    "        \n",
    "cv2.imwrite(\"./results/classiferCNN/detected2_NMS.png\", satimage_nms)\n",
    "\n",
    "print('Save NMS results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
